{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2358d84",
   "metadata": {},
   "source": [
    "# Machine Learning in Finance\n",
    "\n",
    "Machine Learning is a subset of Artifical Intelligence.\n",
    "\n",
    "Inspired by different fields, for example:\n",
    "* Statistics;\n",
    "* Computer Science;\n",
    "* Psychology;\n",
    "* Biology.\n",
    "\n",
    "Is Machine Learning new field of research?\n",
    "\n",
    "Application and benefits of Machine Learning to finance:\n",
    "* Financial Operations (trades) - reduces costs and increases speed;\n",
    "* Financial Decisions (loans, investments) - objective, comply with laws and regulations;\n",
    "* Financial Forecasting\n",
    "* Portfolio Management\n",
    "* Using data to predict or/and find patterns.\n",
    "\n",
    "\n",
    "**Suggested Literature.**\n",
    "1. Trevor Hastie, Robert Tibshirani, Jerome Friedman - *The elements of statistical learning_ Data mining, inference, and prediction*\n",
    "2. Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani - *An Introduction to Statistical Learning with Applications in R*\n",
    "3. Ian Goodfellow and Yoshua Bengio and Aaron Courville - *Deep Learning - Adaptive Computation and Machine Learning*\n",
    "4. John Hull - *Machine Learning in Business - An Introduction to the World of Data Science*\n",
    "5. Marcos Lopez de Prado - *Advances in Financial Machine Learning*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb424f",
   "metadata": {},
   "source": [
    "## Exploring & Preparing Data\n",
    "\n",
    "Import the data. Do Exploratory Data Analysis (EDA). Plot raw data, histograms, scatter plots, correlations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from yahoo finance\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "omxs30_df = yf.download('^OMX', period = \"max\", interval = \"1d\")\n",
    "volv_df = yf.download('VOLV-B.ST', period = \"max\", interval = \"1d\")\n",
    "abb_df = yf.download('ABB.ST', period = \"max\", interval = \"1d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Examine the Volvo stock & OMXS30 Index\n",
    "\n",
    "print(volv_df.head()) \n",
    "print(omxs30_df.head()) \n",
    "\n",
    "\n",
    "# Plot the Adj Close columns for OMX and VOLV-B.ST\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "omxs30_df['Adj Close'].plot(label='OMX', legend=True)\n",
    "volv_df['Adj Close'].plot(label='Volvo', legend=True, secondary_y=True)\n",
    "plt.show() \n",
    "plt.clf()  \n",
    "\n",
    "\n",
    "# Histogram of the daily price change percent of Adj Close for Volvo\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "volv_df['Adj Close'].pct_change().plot.hist(bins=50)\n",
    "plt.xlabel('adjusted close 1-day percent change')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07248fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation (Pearson Correlation Coefficient)\n",
    "\n",
    "\n",
    "# Create 5-day % changes of Adj Close for the current day, and 5 days in the future\n",
    "\n",
    "volv_df['5d_future_close'] = volv_df['Adj Close'].shift(-5)\n",
    "volv_df['5d_close_future_pct'] = volv_df['5d_future_close'].pct_change(5)\n",
    "volv_df['5d_close_pct'] = volv_df['Adj Close'].pct_change(5)\n",
    "\n",
    "\n",
    "# Calculate the correlation matrix between the 5d close pecentage changes (current and future)\n",
    "\n",
    "corr = volv_df[['5d_close_pct', '5d_close_future_pct']].corr()\n",
    "print(corr)\n",
    "\n",
    "\n",
    "# Scatterplot of Adj Close vs 5d_future_close\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(volv_df['Adj Close'], volv_df['5d_future_close'],  s = 3)\n",
    "plt.xlabel(\"Adj Close\")\n",
    "plt.ylabel(\"5d_future_close\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Scatterplot of 5d_close_future_pct vs 5d_future_close\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(volv_df['5d_close_future_pct'], volv_df['5d_close_pct'],  s = 3)\n",
    "plt.xlabel(\"5d_close_future_pct\")\n",
    "plt.ylabel(\"5d_close_pct\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b29a7c",
   "metadata": {},
   "source": [
    "## Targets and Features\n",
    "\n",
    "**Discussion:** What would be our target and features?\n",
    "\n",
    "\n",
    "(Simple) Moving Average. Relative Strenth Index (overbought or oversold).\n",
    "\n",
    "$\\text{RSI} = 100 - \\dfrac{100}{1 + \\dfrac{\\text{average of upward price change}}{\\text{average of downward price change}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10411996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Simple) Moving Average, Relative Strength Index\n",
    "# Do we want to predict raw stock prices or % chaanges?\n",
    "\n",
    "\n",
    "import talib\n",
    "\n",
    "feature_names = ['5d_close_pct'] # a list of the feature names for later\n",
    "\n",
    "\n",
    "# Create moving averages and rsi for timeperiods of 14, 30, 50, and 200\n",
    "\n",
    "for n in [14, 30, 50, 200]:\n",
    "\n",
    "    # Create the moving average indicator and divide by Adj_Close\n",
    "    volv_df['ma' + str(n)] = talib.SMA(volv_df['Adj Close'].values,\n",
    "                              timeperiod=n) / volv_df['Adj Close']\n",
    "    # Create the RSI indicator\n",
    "    volv_df['rsi' + str(n)] = talib.RSI(volv_df['Adj Close'].values, timeperiod=n)\n",
    "    \n",
    "    # Add rsi and moving average to the feature name list\n",
    "    feature_names = feature_names + ['ma' + str(n), 'rsi' + str(n)]\n",
    "\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlations\n",
    "\n",
    "\n",
    "# Drop all na values\n",
    "volv_df = volv_df.dropna()\n",
    "\n",
    "\n",
    "# Create features and targets\n",
    "# use feature_names for features; '5d_close_future_pct' for targets\n",
    "features = volv_df[feature_names]\n",
    "targets = volv_df['5d_close_future_pct']\n",
    "\n",
    "\n",
    "# Create DataFrame from target column and feature columns\n",
    "feature_and_target_cols = ['5d_close_future_pct'] + feature_names\n",
    "feat_targ_df = volv_df[feature_and_target_cols]\n",
    "\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = feat_targ_df.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc1863",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check correlations using heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Plot heatmap of correlation matrix\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "sns.heatmap(corr, annot=True, annot_kws = {\"size\": 10})\n",
    "plt.yticks(rotation=0, size = 12); plt.xticks(rotation=90, size = 12)  # fix ticklabel directions and size\n",
    "plt.tight_layout()  # fits plot area to the plot, \"tightly\"\n",
    "plt.show()  # show the plot\n",
    "plt.clf()  # clear the plot area\n",
    "\n",
    "\n",
    "# Create a scatter plot of the most highly correlated variable with the target\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(volv_df['5d_close_future_pct'], volv_df['ma200'], s = 3)\n",
    "plt.xlabel(\"5d_close_future_pct\")\n",
    "plt.ylabel(\"ma200\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8937e3e",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "Separate data into two sets - Train and Test. Usually, you want to separate into training, validation, and test sets.\n",
    "\n",
    "**Discussion:** How to choose training and test data for our example?\n",
    "\n",
    "Focus on the coefficient of determination $R^2$ and $p-$values.\n",
    "\n",
    "**Discussion:** What are linear models? Discuss examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose training and test data\n",
    "# There are packages that randomly choose training and test data\n",
    "\n",
    "\n",
    "# Import the statsmodels.api library with the alias sm\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Add a constant to the features\n",
    "\n",
    "linear_features = sm.add_constant(features)\n",
    "\n",
    "\n",
    "# Create a size for the training set that is 85% of the total number of samples\n",
    "# For time series data, it is good to choose the first x% as training data and the rest as test data\n",
    "\n",
    "train_size = int(0.85 * targets.shape[0])\n",
    "train_features = linear_features[:train_size]\n",
    "train_targets = targets[:train_size]\n",
    "test_features = linear_features[train_size:]\n",
    "test_targets = targets[train_size:]\n",
    "print(linear_features.shape, train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93defbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear model\n",
    "# Discuss how the fitted model would look like\n",
    "\n",
    "# Create the linear model and complete the least squares fit\n",
    "\n",
    "model = sm.OLS(train_targets, train_features)\n",
    "results = model.fit()  # fit the model\n",
    "print(results.summary())\n",
    "\n",
    "\n",
    "# examine pvalues\n",
    "# Features with p <= 0.05 are typically considered significantly different from 0\n",
    "\n",
    "print(results.pvalues)\n",
    "\n",
    "# Make predictions from our model for train and test sets\n",
    "\n",
    "train_predictions = results.predict(train_features)\n",
    "test_predictions = results.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ff4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Scatter the predictions vs the targets\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(train_predictions, train_targets, alpha=0.2, color='b', label='train', s=6)\n",
    "plt.scatter(test_predictions, test_targets, alpha=0.2, color='r', label='test', s=6)\n",
    "\n",
    "\n",
    "# Plot the prediction line\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot(np.arange(xmin, xmax, 0.01), np.arange(xmin, xmax, 0.01), c='k')\n",
    "#plt.xlim([-0.02,0.02])\n",
    "#plt.ylim([-0.15,0.15])\n",
    "\n",
    "\n",
    "# Label axis\n",
    "\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('actual')\n",
    "plt.legend()  # show the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining features, feature interactions, dimension reduction \n",
    "# Volume as feature\n",
    "\n",
    "\n",
    "# Create 2 new volume features, 1-day % change and 5-day SMA of the % change\n",
    "\n",
    "new_features = ['Volume_1d_change', 'Volume_1d_change_SMA']\n",
    "feature_names.extend(new_features)\n",
    "\n",
    "volv_df['Volume_1d_change'] = volv_df['Volume'].pct_change()\n",
    "volv_df['Volume_1d_change_SMA'] = talib.SMA(volv_df['Volume_1d_change'].values,\n",
    "                        timeperiod=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa6d6b",
   "metadata": {},
   "source": [
    "## Dummy variables\n",
    "\n",
    "**Discussion:** Does the day of the week matter? Does any other specific date or day metter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac379ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy variables for the day of the week\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Use pandas' get_dummies function to get dummies for day of the week\n",
    "\n",
    "days_of_week = pd.get_dummies(volv_df.index.dayofweek,\n",
    "                              prefix='weekday',\n",
    "                              drop_first=True)\n",
    "\n",
    "\n",
    "# Set the index as the original dataframe index for merging\n",
    "\n",
    "days_of_week.index = volv_df.index\n",
    "\n",
    "\n",
    "# Join the dataframe with the days of week dataframe\n",
    "\n",
    "volv_df = pd.concat([volv_df, days_of_week], axis=1)\n",
    "\n",
    "\n",
    "# Add days of week to feature names\n",
    "\n",
    "feature_names.extend(['weekday_' + str(i) for i in range(1, 5)])\n",
    "volv_df.dropna(inplace=True)  # drop missing values in-place\n",
    "print(volv_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "\n",
    "\n",
    "# Add the weekday labels to the new_features list\n",
    "\n",
    "new_features.extend(['weekday_' + str(i) for i in range(1, 5)])\n",
    "\n",
    "\n",
    "# Plot the correlations between the new features and the targets\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "sns.heatmap(volv_df[new_features + ['5d_close_future_pct']].corr(), annot=True, annot_kws = {\"size\": 10})\n",
    "plt.yticks(rotation=0, size = 12)  # ensure y-axis ticklabels are horizontal\n",
    "plt.xticks(rotation=90, size = 12)  # ensure x-axis ticklabels are vertical\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b867f",
   "metadata": {},
   "source": [
    "## Decision Trees \n",
    "\n",
    "**Discussion:** What is classification?\n",
    "\n",
    "**Discussion:** Discuss an example.\n",
    "\n",
    "Decision trees use reduction of variance/spread of data to decide on the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a decision tree\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Create a decision tree regression model with default arguments\n",
    "\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "\n",
    "\n",
    "# Fit the model to the training features and targets\n",
    "\n",
    "decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "\n",
    "# Check the score on train and test\n",
    "\n",
    "print(decision_tree.score(train_features, train_targets))\n",
    "print(decision_tree.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing hyperparameter - max depths\n",
    "\n",
    "\n",
    "# Loop through a few different max depths and check the performance\n",
    "\n",
    "for d in [3,4,5,6,10]:\n",
    "    # Create the tree and fit it\n",
    "    decision_tree = DecisionTreeRegressor(max_depth=d)\n",
    "    decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "    # Print out the scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(decision_tree.score(train_features, train_targets))\n",
    "    print(decision_tree.score(test_features, test_targets), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking results\n",
    "\n",
    "\n",
    "# Use the best max_depth \n",
    "\n",
    "decision_tree = DecisionTreeRegressor(max_depth=3)\n",
    "decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "\n",
    "# Predict values for train and test\n",
    "\n",
    "train_predictions = decision_tree.predict(train_features)\n",
    "test_predictions = decision_tree.predict(test_features)\n",
    "\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(train_predictions, train_targets, label='train', s = 5)\n",
    "plt.scatter(test_predictions, test_targets, label='test', s = 5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76273a1a",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Reducing variance of Decision Trees as a collection of (independent) Decision Trees.\n",
    "\n",
    "Bootstrap aggregating (bootstrap sample from the training sample).\n",
    "\n",
    "Sampling features at each split (taking a few instead of all features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Create the random forest model and fit to the training data\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=200)\n",
    "rfr.fit(train_features, train_targets)\n",
    "\n",
    "# Look at the R^2 scores on train and test\n",
    "\n",
    "print(rfr.score(train_features, train_targets))\n",
    "print(rfr.score(test_features, test_targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332b301",
   "metadata": {},
   "source": [
    "## Random Forest Hyperparameters\n",
    "\n",
    "**n_estimators** - number of trees in the random forest\n",
    "\n",
    "**max_depth** - total number of splits in trees\n",
    "\n",
    "**max_features** - maximum number of features chosen at splits, square root of total number of features\n",
    "\n",
    "**random_state** - to have reproducable results \n",
    "\n",
    "Sklearn's GridSearchCV() can also help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Grid - find out random forest hyperparameters\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "# Create a dictionary of hyperparameters to search\n",
    "\n",
    "grid = {'n_estimators': [200], 'max_depth': [3], 'max_features': [4, 8], 'random_state': [42]}\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "# Loop through the parameter grid, set the hyperparameters, and save the scores\n",
    "\n",
    "for g in ParameterGrid(grid):\n",
    "    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n",
    "    rfr.fit(train_features, train_targets)\n",
    "    test_scores.append(rfr.score(test_features, test_targets))\n",
    "\n",
    "    \n",
    "# Find best hyperparameters from the test score and print\n",
    "\n",
    "best_idx = np.argmax(test_scores)\n",
    "print(test_scores[best_idx], ParameterGrid(grid)[best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ee882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model performance\n",
    "\n",
    "\n",
    "# Use the best hyperparameters from before to fit a random forest model\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=200, max_depth=3, max_features=4, random_state=42)\n",
    "rfr.fit(train_features, train_targets)\n",
    "\n",
    "\n",
    "# Make predictions with our model\n",
    "\n",
    "train_predictions = rfr.predict(train_features)\n",
    "test_predictions = rfr.predict(test_features)\n",
    "\n",
    "\n",
    "# Create a scatter plot with train and test actual vs predictions\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(train_targets, train_predictions, label='train', s = 5)\n",
    "plt.scatter(test_targets, test_predictions, label='test', s = 5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786dabe",
   "metadata": {},
   "source": [
    "## Random Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee2f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest feature importances\n",
    "\n",
    "\n",
    "# Get feature importances from our random forest model\n",
    "\n",
    "importances = rfr.feature_importances_\n",
    "\n",
    "\n",
    "# Get the index of importances from greatest importance to least\n",
    "\n",
    "sorted_index = np.argsort(importances)[::-1]\n",
    "x = range(len(importances))\n",
    "\n",
    "\n",
    "# Create tick labels \n",
    "\n",
    "labels = np.array(feature_names)[sorted_index]\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.bar(x, importances[sorted_index], tick_label=labels)\n",
    "\n",
    "\n",
    "# Rotate tick labels to vertical\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb80a6",
   "metadata": {},
   "source": [
    "##  Gradient Boosting\n",
    "\n",
    "Sequential - e.g., one by one Decision Tree starting with features, then fitting residuals.\n",
    "\n",
    "Using negative derivative of loss function (direction).\n",
    "\n",
    "If linear models are Toyota Camry, Gradient Boosting is a Black Hawk helicopter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A gradient boosting model\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Create GB model with pre-determined hyperparameters\n",
    "\n",
    "gbr = GradientBoostingRegressor(max_features=4,\n",
    "                                learning_rate=0.01,\n",
    "                                n_estimators=200,\n",
    "                                subsample=0.6,\n",
    "                                random_state=42)\n",
    "gbr.fit(train_features, train_targets)\n",
    "\n",
    "print(gbr.score(train_features, train_targets))\n",
    "print(gbr.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient boosting feature importances\n",
    "\n",
    "\n",
    "# Extract feature importances from the fitted gradient boosting model\n",
    "\n",
    "feature_importances = gbr.feature_importances_\n",
    "\n",
    "\n",
    "# Get the indices of the largest to smallest feature importances\n",
    "\n",
    "sorted_index = np.argsort(feature_importances)[::-1]\n",
    "x = range(len(feature_importances))\n",
    "\n",
    "\n",
    "# Create tick labels \n",
    "\n",
    "labels = np.array(feature_names)[sorted_index]\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.bar(x, feature_importances[sorted_index], tick_label=labels)\n",
    "\n",
    "\n",
    "# Set the tick lables to be the feature names, according to the sorted feature_idx\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341c282",
   "metadata": {},
   "source": [
    "## Standardizing Data\n",
    "\n",
    "You can use standard standardizing data instead of sklearn's scale.\n",
    "\n",
    "**Discussion:** Why do we standardize data? Should we remove unimportant features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b034dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing data\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "# Remove unimportant features (weekdays)\n",
    "\n",
    "train_features = train_features.iloc[:, :-4]\n",
    "test_features = test_features.iloc[:, :-4]\n",
    "\n",
    "\n",
    "# Standardize the train and test features\n",
    "\n",
    "scaled_train_features = scale(train_features)\n",
    "scaled_test_features = scale(test_features)\n",
    "\n",
    "\n",
    "# Plot histograms of the 14-day SMA RSI before and after scaling\n",
    "\n",
    "f, ax = plt.subplots(nrows=2, ncols=1)\n",
    "train_features.iloc[:, 2].hist(ax=ax[0])\n",
    "ax[1].hist(scaled_train_features[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7f493",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f76424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize a hyperparameter\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "for n in range(2,13,1):\n",
    "    # Create and fit the KNN model\n",
    "    knn = KNeighborsRegressor(n_neighbors=n)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    knn.fit(scaled_train_features, train_targets)\n",
    "    \n",
    "    # Print number of neighbors and the score to find the best value of n\n",
    "    print(\"n_neighbors =\", n)\n",
    "    print('train, test scores')\n",
    "    print(knn.score(scaled_train_features, train_targets))\n",
    "    print(knn.score(scaled_test_features, test_targets))\n",
    "    print()  # prints a blank line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "\n",
    "\n",
    "# Create the model with the best-performing n_neighbors of 12\n",
    "\n",
    "knn = KNeighborsRegressor(12)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "knn.fit(scaled_train_features, train_targets)\n",
    "\n",
    "\n",
    "# Get predictions for train and test sets\n",
    "\n",
    "train_predictions = knn.predict(scaled_train_features)\n",
    "test_predictions = knn.predict(scaled_test_features)\n",
    "\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(train_predictions, train_targets, label='train', s = 5)\n",
    "plt.scatter(test_predictions, test_targets, label='test', s = 5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1efa67",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Similar to other models (features + targets -> predictions).\n",
    "\n",
    "Grown with development of GPU and software improvement.\n",
    "\n",
    "Inspired by human brain. Multiple layers each with multiple neorons. Each neuron has math behind it, then apply activation function to add non-linearity. Common activation - ReLU. Loss function to compare prediction and targets (e.g., Mean Square Error).\n",
    "\n",
    "First go forward to a prediction. Then backward (back propagation) using error from the loss function. (updating weights and biases using derivatives -gradient descent)\n",
    "\n",
    "Widely used - image recognition, stock market prediction, healthcare, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84890f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit first neural network\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))\n",
    "model_1.add(Dense(20, activation='relu'))\n",
    "model_1.add(Dense(1, activation='linear')) # linear for regression (can be classification)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "model_1.compile(optimizer='adam', loss='mse')\n",
    "history = model_1.fit(scaled_train_features, train_targets, epochs=25) # number of epochs is the number of training cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19539e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.plot(history.history['loss'])\n",
    "\n",
    "\n",
    "# Use the last loss as the title\n",
    "\n",
    "plt.title('loss:' + str(round(history.history['loss'][-1], 6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090173e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Calculate R^2 score\n",
    "\n",
    "train_preds = model_1.predict(scaled_train_features)\n",
    "test_preds = model_1.predict(scaled_test_features)\n",
    "print(r2_score(train_targets, train_preds))\n",
    "print(r2_score(test_targets, test_preds))\n",
    "\n",
    "\n",
    "# Plot predictions vs actual\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(train_preds, train_targets, label='train', s = 5)\n",
    "plt.scatter(test_preds,test_targets,label='test', s= 5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function - might be useful to meet needs depending on task\n",
    "# In our stock/index example, direction is important\n",
    "\n",
    "import keras.losses\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Create loss function for our case (to predict correct direction).\n",
    "# add penalty where the direction is wrong\n",
    "\n",
    "def sign_penalty(y_true, y_pred):\n",
    "    penalty = 100.\n",
    "    loss = tf.where(tf.less(y_true * y_pred, 0), \n",
    "                     penalty * tf.square(y_true - y_pred),  \n",
    "                     tf.square(y_true - y_pred)) \n",
    "\n",
    "    return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "keras.losses.sign_penalty = sign_penalty  # enable use of loss with keras\n",
    "print(keras.losses.sign_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea59ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit neural network with the previously created loss function\n",
    "\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))\n",
    "model_2.add(Dense(20, activation='relu'))\n",
    "model_2.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "# Fit the model with our custom 'sign_penalty' loss function\n",
    "\n",
    "model_2.compile(optimizer='adam', loss='sign_penalty')\n",
    "history = model_2.fit(scaled_train_features, train_targets, epochs=10)\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('loss:' + str(round(history.history['loss'][-1], 6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance\n",
    "\n",
    "\n",
    "# Evaluate R^2 scores\n",
    "\n",
    "train_preds = model_2.predict(scaled_train_features)\n",
    "test_preds = model_2.predict(scaled_test_features)\n",
    "print(r2_score(train_targets, train_preds))\n",
    "print(r2_score(test_targets,test_preds))\n",
    "\n",
    "\n",
    "# Scatter the predictions vs actual \n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(train_preds, train_targets, label='train', s=5)\n",
    "plt.scatter(test_preds, test_targets, label='test', s=5)  # plot test set\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4dd7d",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "There are many different ways to deal with overfitting, for example:\n",
    "* dropout;\n",
    "* ensembling;\n",
    "* L1/L2 regulation;\n",
    "* Early stopping;\n",
    "* Adding noise.\n",
    "\n",
    "Dropout randomly drops a fraction of neurons during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a41e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using a dropout\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "# Create model with dropout\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))\n",
    "model_3.add(Dropout(0.2))\n",
    "model_3.add(Dense(20, activation='relu'))\n",
    "model_3.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "# Fit model with mean squared error loss function\n",
    "\n",
    "model_3.compile(optimizer='adam', loss='mse')\n",
    "history = model_3.fit(scaled_train_features, train_targets, epochs=25)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('loss:' + str(round(history.history['loss'][-1], 6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembling models (similarly to random forest - ensembling many decision trees)\n",
    "# Could combine different models\n",
    "\n",
    "# Make predictions from the 3 neural net models\n",
    "\n",
    "train_pred1 = model_1.predict(scaled_train_features)\n",
    "test_pred1 = model_1.predict(scaled_test_features)\n",
    "\n",
    "train_pred2 = model_2.predict(scaled_train_features)\n",
    "test_pred2 = model_2.predict(scaled_test_features)\n",
    "\n",
    "train_pred3 = model_3.predict(scaled_train_features)\n",
    "test_pred3 = model_3.predict(scaled_test_features)\n",
    "\n",
    "\n",
    "# Horizontally stack predictions and take the average across rows\n",
    "\n",
    "train_preds = np.mean(np.hstack((train_pred1, train_pred2, train_pred3)), axis=1)\n",
    "test_preds = np.mean(np.hstack((test_pred1,test_pred2,test_pred3)), axis=1)\n",
    "print(test_preds[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance model with ensembling \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Evaluate the R^2 scores\n",
    "\n",
    "print(r2_score(train_targets, train_preds))\n",
    "print(r2_score(test_targets, test_preds))\n",
    "\n",
    "\n",
    "# Scatter the predictions vs actual \n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "plt.scatter(train_preds, train_targets, label='train', s=5)\n",
    "plt.scatter(test_preds, test_targets, label='test',s=5)\n",
    "plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
